The ability to learn concepts about a novel task, such as the goal and motion plans, from a few demonstrations is a crucial building block for intelligent agents – it allows an agent to learn to perform new tasks from other agents (including humans) from little data.
Humans, even from a young age, can learn various new tasks from little data and generalize what they learned to perform these tasks in new situations
In machine learning and robotics, this class of problems is referred to as Few-Shot Learning
. Despite being a widely studied problem, it remains unclear how we can enable machine learning models to learn concepts of a novel task from only a few demonstrations and generalize the concepts to new situations, just like humans do. Common approaches learn policies either directly, which often suffer from covariate shift
, which are largely limited to previously seen behavior
In a different vein, other work has relied on pretraining on task families and assumes that task learning corresponds to learning similar tasks to ones already seen in the task family
Inspired by the success of generative modeling in few-shot visual concept learning
, where concepts are latent representations, in this work, we investigate whether and how few-shot task concept learning can benefit from generative modeling as well. Learning concepts from sequential demonstrations rather than images is by nature more challenging due to sequential data often not satisfying the i.i.d. assumption in machine learning
. In particular, we assume access to a large pretraining dataset of paired behaviors and task representations to learn a conditional generative model that synthesizes trajectories conditioned on task descriptions. We hypothesize that by learning a generative model conditioned on explicit representations of behavior, we can acquire strong priors about the nature of behaviors in these domains, enabling us to more effectively learn new behavior that is not within the pretraining distribution, given a limited number of demonstrations, and further generate the learned behavior in new settings.
(a latent representation of the task), we train a generative model
to generate behavior from a concept. Then, given demonstrations of a new behavior
, ‘jumping jacks’) without its concept label, we aim to learn its concept representation by optimizing concept
We extensively evaluate our approach for various domains.
To this end, we propose Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM). In our approach, we first pretrain a large conditional generative model which synthesizes different trajectories conditioned on different task descriptions. To learn new tasks from a limited number of demonstrations, we then formulate few-shot task learning as an
, where we find the latent task description, which we refer to as a
, which maximizes the likelihood of generating the demonstrations. This approach allows us to leverage the powerful task priors learned by the generative model to learn the shared concepts between demonstrations without finetuning the model (
New concepts are either (1) compositions of training concepts (
, multiple desired relations between objects that define a new object rearrangement concept) or (2) new concepts that are not explicit compositions in the natural language symbolic space of training concepts (
, a new human motion ‘jumping jacks’ is not an explicit composition of training concepts ‘walk’, ‘golf’ etc.). Thanks to generative models’ compositional properties that enable compositional concept learning
, in addition to being able to learn a single concept from demonstrations directly, FTL-IGM learns compositions of concepts from demonstrations that, when combined, describe the new concept.
We show that our approach generates diverse trajectories encapsulating the learned concept. We achieve this due to two properties of generative models. First, these models have shown strong interpolation abilities
, which allow generating the new concept on new initial states they were not demonstrated from. Second, these models have compositional properties that enable compositional trajectory generation
, which allow composing learned concepts with training concepts to synthesize novel behavior that was not demonstrated (
, ‘jumping jacks’ and ‘walk’), see 
. We further demonstrate that our approach addresses a unique challenge introduced in learning task concepts: we utilize plans generated by learned concepts in a closed-loop fashion.
Our main contributions are (1) formulating the problem of task learning from few demonstrations as Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM), (2) adapting a method for efficient concept learning to this problem based on the new formulation, and (3) a systematic evaluation revealing the ability of our method to learn new concepts across a diverse set of domains.
Our problem setting is closely related to learning from few demonstrations. There has been much work on learning to generate agent behavior given few demonstrations. There are several common approaches to this problem. First, behavior cloning (
) is a supervised learning method to learn a policy from demonstrations that predicts actions from states. Similar to our framework, goal-conditioned BC can predict states from task representations and states
. Finetuning these models to learn new behaviors requires labeled demonstrations of the new task. We assume unlabeled demonstrations. BC often suffers from covariate shift
and fails to generate the demonstrated behavior in novel scenarios. This can be mitigated by assuming access to a human in the loop
. Second, the inverse reinforcement learning (
) framework learns a policy that maximizes the return of an explicitly
learned reward function. These works learn a reward for a single task or for a set of tasks (
). While IRL is more data efficient than BC, it is computationally costly due to learning a policy every iteration via an inner reinforcement learning (RL) loop. Additionally, it requires access to taking actions in the environment during training and when faced with a new task, we have to retrain the reward again. A third approach is
, which can robustly infer concepts such as goals and beliefs even in unseen scenarios. However, it assumes access to a planner, knowledge about environment dynamics, and the task/goal space. Finally,
learn actions in a supervised manner by representing the task with demonstrations. This allows few-shot generalization without further training.
In contrast, we do not learn an action-generating policy directly or via a reward function. For concept learning, we do not assume having access to any given planner, world model, actions, rewards, or prior over the task space. Instead, we learn
(task representations) from demonstrations via a pretrained generative model that takes a concept as input and directly produces state sequences. We then input the learned concept into the generative model to produce behavior similar yet diverse to the demonstrated one. We further demonstrate how to use these state sequences with a planner to take actions and achieve the desired behavior. The idea of concept learning via generative models has been explored for computer vision applications
. We build on this work and show how to extend it to learn agent task concepts. Our work also differs from prior works on learning trajectory representations
. These works focus on learning plans over trajectory embeddings, whereas we learn a task representation from demonstrations on which we condition to generate behavior.
There has been work on generative modeling for decision-making, including generative models for single-agent behaviors, such as implicit BC
, and for multi-agent motion prediction such as
. The success of diffusion policy in predicting sequences of future actions has led to 3D extensions
, and combined with ongoing robotic data collection efforts
and advanced vision and language models, has led to vision-language-action generative models
. In this work, we utilize a conditional generative model for the
There has been work on obtaining composable data representations.
learns unsupervised disentangled representations for images. MONet
decompose visual scenes via segmentation masks and COMET
via energy functions. There is also work on composing representations to generate data with composed concepts. Generative models can be composed together to generate visual concepts
. The generative process can also be altered to generate compositions of visual
concepts. We aim to obtain task concepts and generate them in composition with other task concepts.
Inspired by recent success in large generative models, we propose a generative formulation for learning specific behavior given a small set of demonstrations, which we term Few-Shot Task Learning through Inverse Generative Modeling
. In our formulation, we assume access to a large pretraining dataset
italic_D start_POSTSUBSCRIPT pretrain end_POSTSUBSCRIPT = { ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT
italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … } ⊆ caligraphic_T
italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ caligraphic_C ⊆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT
describing trajectories. This assumption is often not prohibitive in practice. There is typically a vast amount of existing data collected from the internet or prior exploration in an environment, which may only need to be weakly annotated to characterize the trajectory,
, we learn a conditional generative model
caligraphic_G start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT : caligraphic_C × caligraphic_S → caligraphic_T
conditioned on concepts and initial states, which learns to generate future trajectories. We train the parameters of
start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_τ , italic_c ∼ italic_D start_POSTSUBSCRIPT pretrain end_POSTSUBSCRIPT end_POSTSUBSCRIPT 
Then, given an unlabeled demonstration dataset
italic_D start_POSTSUBSCRIPT new end_POSTSUBSCRIPT ∼ caligraphic_D
, we formulate learning a new concept
that is used to sample trajectories from
the generative model. In particular, we learn new concept
so that our frozen conditional generative model
maximizes the likelihood of trajectories in
start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT over~ start_ARG italic_c end_ARG end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_τ ∼ italic_D start_POSTSUBSCRIPT new end_POSTSUBSCRIPT end_POSTSUBSCRIPT 
. We find that this design choice enables us to leverage the priors learned by
given very few demonstrations, even if the demonstrated
. For evaluation in closed loop, we further assume access to a planner that given two states plans which action to take in the environment, sometimes via access to simulation in the environment. We use this planner sequentially to make decisions in the environment.
We generate versions of the new behavior conditioned on the learned concept and (1) new initial states and (2) composed with other concepts.
The key difference between our approach to few-shot adaptation from demonstrations and prior approaches is the assumption and usage of a large pretraining dataset of paired behaviors and concepts
combined with an invertible generative model. We learn new concepts solely from demonstrations without finetuning model weights or taking actions in the environment by relying on the pretrained concept space.
Few-Shot Concept Learning Based on FTL-IGM
We adapt a few-shot concept learning method to task concepts based on the FTL-IGM framework. During training we learn a generative model
{ ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT
, and given demonstrations of a new task
{ over~ start_ARG italic_τ end_ARG } start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT
to produce the new behavior via
. We then generate a diverse set of behaviors via
, either for the learned concept
conditioned on new initial states or for compositions of
Training a diffusion model to generate behavior
is a generative model that given a forward noise adding process
italic_q ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ) := caligraphic_N ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; square-root start_ARG 1 - italic_β start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , italic_β start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_I )
italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_β start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT
italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) := caligraphic_N ( italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; italic_μ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) , roman_Σ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) )
simplify the training objective to estimate noise
blackboard_E start_POSTSUBSCRIPT italic_t ∼ caligraphic_U { 1 , italic_T } , italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_ϵ ∼ caligraphic_N ( 0 , bold_I ) end_POSTSUBSCRIPT 
by the forward noising process at diffusion step
italic_q ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) := caligraphic_N ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; square-root start_ARG over¯ start_ARG italic_α end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , ( 1 - over¯ start_ARG italic_α end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) bold_I )
over¯ start_ARG italic_α end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT := roman_Π start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT ( 1 - italic_β start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )
by guiding the reverse noising process with classifier gradients. The noise prediction becomes
over^ start_ARG italic_ϵ end_ARG = italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) - italic_ω square-root start_ARG 1 - over¯ start_ARG italic_α end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ∇ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t )
italic_p start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t )
is trained on noisy images, and
that achieves the same objective without the need for training a separate classifier. This is done by learning a conditional and unconditional model by removing the conditioning information with dropout during training. The noise prediction is then
over^ start_ARG italic_ϵ end_ARG = italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) + italic_ω ( italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y , italic_t ) - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) )
demonstrate how this idea can be used to generate images conditioned on a class. Diffusion models have recently shown success as generative models for
used a conditional classifier-free guidance diffusion model
to generate trajectories of future states to reach given an input observation. We adopt this objective and learn a denoising model
conditioned on latent concepts and initial observed states to estimate noise of a future state trajectory:
blackboard_E start_POSTSUBSCRIPT ( italic_τ , italic_c ) ∼ italic_D start_POSTSUBSCRIPT pretrain end_POSTSUBSCRIPT , italic_ϵ ∼ caligraphic_N ( 0 , bold_I ) , italic_t ∼ caligraphic_U { 1 , italic_T } , italic_γ ∼ roman_Bern ( italic_p ) end_POSTSUBSCRIPT 
is the probability of removing conditioning information which is then replaced by dummy condition
is the initial state corresponding to trajectory
italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_τ )
italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_τ
by the forward noising process. We then extend this approach for the inverse problem, namely, learning a concept from demonstrations.
use a frozen generative model to
representations from few images depicting the concept by optimizing the model’s input
italic_v start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT = roman_arg roman_min start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_v , italic_ϵ ∼ caligraphic_N ( 0 , 1 ) , italic_t end_POSTSUBSCRIPT 
with a pretrained diffusion model in an unsupervised manner. Namely, from a set of images that depict various concepts, for each image
they learn a set of weights
italic_ω start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT
and a shared set of visual concepts for all images
over^ start_ARG italic_ϵ end_ARG = italic_ϵ ( italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) + ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_ω start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_ϵ ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_t ) - italic_ϵ ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_t ) )
. We extend these formulations to inferring multiple concepts, whose composition describes a single task concept, from few demonstrations of a task.
and demonstrations of a new concept
{ over~ start_ARG italic_τ end_ARG } start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT
{ over~ start_ARG italic_c end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , over~ start_ARG italic_c end_ARG start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT }
{ italic_ω start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_ω start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT }
that best describe the demonstrations. Starting from uniformly sampled concept embeddings
over~ start_ARG italic_c end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∼ caligraphic_U (  start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT )
over~ start_ARG italic_c end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT
blackboard_E start_POSTSUBSCRIPT italic_ϵ ∼ caligraphic_N ( 0 , bold_I ) end_POSTSUBSCRIPT  .
We find that this compositional approach enables us to effectively represent and learn new demonstrations, even when demonstrations are substantially different than those seen in training tasks.
over~ start_ARG italic_c end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT
, whose composition describes the new task
, we evaluate the behavior it generates by initializing
italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_τ ) ∼ caligraphic_N ( 0 , italic_α bold_I )
italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∼ caligraphic_N ( italic_μ start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , italic_α roman_Σ start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT )
iteratively as a function of the estimated denoising function
over^ start_ARG italic_ϵ end_ARG ( italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT )
are the mean and variance that define the reverse process, and
italic_α ∈ [ 0 , 1 )
is a scaling factor that leads to lower temperature samples, until generating
italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_τ
representing the trajectory of the agent. The denoising function is constructed by fixed or learned weights as defined in Eq.
and by any number of concepts
. The applications of the generation procedure can be summarized as:
Learned concept and demonstrated initial states.
We apply our learned concept to a set of demonstrated initial states. In domains where the initial state and concept jointly determine optimal behavior, the generated trajectory corresponds to optimal actions to execute (
goal-oriented navigation). In contrast to other domains where the initial state is irrelevant for a task due to the randomness in sampling
italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_τ )
motion capture), generated trajectories correspond to diverse plausible behaviors exhibiting the learned concept.
Learned concept and novel initial states.
We further apply the generation procedure from our learned concept on novel initial states, to generate trajectories of new behaviors exhibiting our conditioned concept. Prior methods may suffer from covariate shift in this setting
. We empirically show that our method is less prone to this problem.
Learned concept composed with other concepts.
Finally, we modify the generation procedure of our newly learned concept to generate trajectories that simultaneously exhibit other concepts. To generate a trajectory with an added another concept, we add another term to the sum in Eq.
where the learned concept is composed with a training concept
italic_ω start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_τ ) , italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_t ) - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_τ ) , italic_c start_POSTSUBSCRIPT ∅ end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_t ) )
. This modified generation procedure constructs trajectories which exhibits behavior that has a composition of the learned concept and the other specified concepts
, in environments where an inverse dynamics model is provided, we generate trajectories in a closed loop. We execute actions calculated by the inverse dynamics given the predicted plan by the model and then repeatedly replan given new observations.
Training concepts are single pairwise relations (‘A right of/above B’), and new concepts are either compositions of training concepts (‘A right of/above B’
‘B right of/above C’) or new relations (‘A diagonal to B’, ‘A, B, C on circle circumference of radius r’).
We demonstrate results in four domains where concept representations are T5
embeddings of task descriptions in natural language for training, and empty string embeddings for the dummy condition. During few-shot concept learning, we are provided with three to five demonstrations of a composition of training concepts or of a novel concept that is not an explicit composition of training tasks in natural language symbolic space. We ask a model to learn the concept from these demonstrations.
Learning concepts describing goals that are spatial relations between objects.
Object rearrangement is a common task in robotics
, serving as a foundation for a broader range of tasks such as housekeeping and manufacturing. Here, we use a 2D object rearrangement domain to evaluate the ability of our method to learn task specification concepts. Given a concept representing a relation between objects, we generate a single state describing that relation. The concept in a training example describes the relation (either ‘right of’ or ‘above’) between only one pair of objects (out of three objects) in the environment. Then, a model must learn compositions of these pairwise relations and new concepts such as ‘diagonal’ and ‘circle’ (see 
demonstrate that our method learns unseen compositions of training concepts and new concepts. They further demonstrate how our method composes new concepts with learned concepts. For additional qualitative results, please refer to Appendix
Object rearrangement new concept qualitative evaluation.
Learning the new concept ‘square diagonal to triangle’ and composing it with the training concept ‘circle right of square’.
While successful in most cases, there are also a few failure examples. The accuracy for the new ‘circle’ concept is low (
) compared to the mean over task types in 
). This is most likely due to this concept lying far out of the training distribution. The task ‘square right of circle
triangle above circle’ has low accuracy for 2 concepts (
). This may arise from the combined concept-weight optimization process – as there is no explicit regularization on weights, they may converge to 0 or diverge. In 
, we show that concept components may or may not capture new concept relations.
Learning concepts describing goals based on attributes of target objects.
We test our method in a goal-oriented navigation domain adapted from the AGENT dataset
, where an agent navigates to one of two potential targets. Conditioned on a concept representing the attributes of the desired target object and initial state, we generate a state-based trajectory describing an agent navigating to the target. Each object has a color and a shape out of four possible colors and four shapes. During training, we provide 16 target-distractor combinations that include all colors and shapes (but not all combinations), and a concept is conditioned on one of the target’s attributes (
, color). We introduce new concepts defined by both target attributes, including (1) unseen color-shape target combinations and (2) new target-distractor combinations. 
shows an example. In training, we see bowl and red object targets. A new concept includes a novel composition as the target – red bowl. The new concept distractor objects (green bowl and red sphere) were introduced during training, but they were not paired with a red bowl as the target. As 
shows, our method successfully learns concepts where targets are new compositions of target attributes in settings with new target-distractor pairs and generalizes to new initial object states. We further evaluate our model and baselines in closed loop (
) by making an additional assumption that a planner is provided. The planner produces an action given a current state and a future desired state predicted by a model.
Unlike prior work on learning to compose human poses from motion capture (MoCap) data
, here we focus on the inverse problem – learning new actions from MoCap data. In particular, we use the
CMU Graphics Lab Motion Capture Database
. We train on various human actions in the database and few-shot learn three novel concepts (see Appendix
, we ask five human volunteers to select generated behaviors that depict training and new concepts. We demonstrate quantitatively that our method generates human motion that captures the desired behavior across training and new behaviors. We qualitatively demonstrate how our method generates learned new concepts (‘jumping jacks’ and ‘breaststroke’) from new initial states and composes ‘jumping jacks’ with training concepts ‘walk’, ‘jump’, and ‘march’.
Object rearrangement (left) and AGENT closed loop (right) quantitative evaluation on training and few-shot novel concept learning.
Accuracy of FTL-IGM (ours), BC, VAE, and In-Context over concept generation of training concepts, novel compositions, novel concepts, and new initial states. We plot the average and standard error over new task types. Full details of the evaluation metrics appear in Appendix
and for baselines implementation in Appendix
Results are best viewed on our website
. We compare motion generated by our method to various baselines on new initial states for ‘jumping jacks’ and ‘breaststroke’. Our method is noisy yet captures the widest range of motion. While other methods often produce smoother trajectories, they mostly capture local (VAE) or degenerate (BC, In-Context, Language) motion.
, an agent acts in a challenging multi-agent environment to complete a driving task. We train on several driving scenarios (‘highway’, ‘exit’, ‘merge’, and ‘intersection’) and learn a new driving scenario (‘roundabout’) from several demonstrations (see 
). We evaluate this scenario in closed loop on new initial states, assuming access to a planner that can simulate taking actions in the environment. Over two evaluation metrics (crash and task completion rate), our method achieves overall best results (
). We evaluate training pushing in closed loop and and achieve success (
accuracy). We evaluate the new concept, elevated pushing, against a baseline that conditions on the training ‘push green circle to orange triangle’ representation in the new scenario setup where the objects are placed on a book. Learning the new concept succeeds (
accuracy) while using the training representation fails (
accuracy) and the robot often pushes the book instead of the object. Details are in Appendix
and qualitative results are on our
In training, targets are defined by a single attribute (color or shape), and in new concepts, targets are defined by a novel combination of color and shape attributes. To make the setting more challenging, distractor objects in new concept demonstrations have a combination of attributes that are within the training distribution.
For training concepts and new concepts on new initial states, we report (top) percentage of time each method is the most successful at depicting a concept and (bottom) percentage of time each method depicts a concept. Mean and standard deviation are calculated over the number of scenarios in each setting and human subjects.
A controlled agent (green) completes various driving objectives as fast as possible while sharing the road with other vehicles (blue) and avoiding collisions. Training concepts: ‘highway’, ‘exit’, ‘merge’, and ‘intersection’, new concept: ‘roundabout’.
Driving crash (left) and success (right) rates.
Crash rate (lower is better) and task completion rate (higher is better) averaged over training tasks. We report standard errors over training tasks and accuracy over
trajectories generated from the learned new concept. VAE has a high completion rate yet a high crash rate. In-context has a low crash rate yet 0% success rate – typically, the controlled vehicle reaches the roundabout’s center but does not complete the crossing. Overall, our method learns to complete the roundabout crossing with competitive crash and success rates.
We compare our method with goal-conditioned behavior cloning (BC), which, given a condition and a state, outputs the next state in a sequence. It is trained on our paired pretraining dataset and learns concepts by optimizing the input condition to maximize the likelihood of new concept demonstration transitions. We test its ability to compose new learned concepts with training concepts naively by adding conditions that are then inputted into the model. We observe that even though goal-conditioned BC has access to the pretrained dataset and conditions, and while it may learn new concepts, it suffers from covariate shift on new initial states and lacks the ability to generalize to novel compositions of the learned concept with training concepts (Figures
). To achieve these generalization capacities, we need a model that can process interpolated (initial states) and composed (concepts) conditions, such as our generative model.
Learning the concept space with a VAE.
We compare our method with VAE
that does not utilize the concepts in the paired pretraining data but rather learns the concept space by reconstructing pretraining data trajectories through their encoded representation
. Trajectories are generated via a decoder conditioned on an initial state and
is obtained by encoding a trajectory for training evaluation and by fixing the decoder and optimizing
to generate a given demonstration when learning a new concept. We find that the VAE model learns a latent space that captures training and new concepts but does not enable generalization to new initial states (Figures
). This highlights the importance of concept representations in the pretrained data.
We compare our approach with training a method to in-context learn from demonstrations. Specifically, we compare our approach to Xu et al.
using the pretrained dataset for few-shot behavior generation without further training. We sequentially predict states from demonstrations of a concept and window of current states and show that our method adapts better to new concepts (Figures
). This emphasizes the need to learn explicit concept representations.
Conditioning on Language Descriptions of New Concepts.
There has been work on generating actions from language instructions
. We demonstrate that in our setup, merely providing new concept language instructions embedded with T5 (as in our pretraining dataset) is insufficient, and generalization is better when learning concepts from few demonstrations. For AGENT, training compositions on new initial states has an average accuracy and standard error of
). For Object Rearrangement, training compositions (
), and new and training concept compositions (
) accuracies are significantly lower than ours (
). For MoCap, we demonstrate qualitatively that instead of capturing new human actions, the agent transitions into walking.
Results are best viewed on our
Learning two concepts yields higher accuracy than one concept
for Object Rearrangement, AGENT, and Driving, and find that, on average, learning two concepts improves concept learning. We demonstrate qualitatively for MoCap that learning two conditions is preferable. In ‘jumping jacks’, we observe that the motion lacks raising and lowering both arms and in ‘breaststroke’, it lacks complete arm and upper torso movement. Results are best viewed on our
Ablation on the number of learned concepts.
We test the effect of the number of learned concepts and weights in FTL-IGM on the generation accuracy of new learned concepts. On average, learning two concept components and their weights is preferable to learning one concept component and its weight. We report average accuracy and standard error over task types for Object Rearrangement and AGENT. Driving includes a single new concept and we report accuracy only.
How are learned new concepts related to training concepts?
New concepts that are compositions of training concepts.
We analyze what the learned two concepts in Object Rearrangement and AGENT learn for novel concept compositions (
, ‘red bowl’). For each concept (
, ‘red’ and ‘bowl’), we generate two sets of
shows accuracy for these sets over the concepts. In some cases (most notably the ‘line’ concept in Object Rearrangement, ‘circle right of triangle and triangle right of square’), each learned component captures a single composed concept. In other cases, a single learned component captures both concepts (
New concepts that are not explicit compositions of training concepts in natural language symbolic space.
embeddings for T5 training concept representations and learned concept component representations. We note that learned components are relatively close to training concepts, maintaining the model’s input distribution, yet capture concepts that are not explicit compositions of training concepts.
t-SNE embeddings of new concepts that are not explicit compositions of training concepts.
See interactive version for detailed labels on our
In this work, we formulate the problem of new task concept learning as Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM). We adapt a method for concept learning based on this new formulation and evaluate task concept learning against baselines in four domains. Our extensive experimental results show that, unlike the baselines, FTL-IGM successfully learns novel concepts from a few examples and generalizes the learned concepts to unfamiliar scenarios. It also composes learned concepts to form unseen behavior thanks to the compositionality of the generative model. These results demonstrate the efficacy, sample efficiency, and generalizability of FTL-IGM.
However, our work has several limitations. First, while our framework is general for any parameterized generative model, our implementation with a diffusion model incurs high inference time. We note that there is still space for improvement in the MoCap generation quality and in the compatibility rate of demonstrations generated by composing learned and training concepts. In addition, we assume that learned concepts lie within the landscape of training concepts to learn them from a few demonstrations without retraining the model. We have approached the question of what new concepts can be represented by compositions of concepts in this landscape empirically, leaving a theoretical analysis as future work. We are hopeful that with the continued progress in the field of generative AI, more powerful pretrained models will become available. Combined with our framework, this will unlock a stronger ability to learn and generalize various task concepts in complex domains.